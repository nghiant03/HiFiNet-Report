Our goal is to detect and label faults in wireless sensor networks (WSNs) from each node’s time-series data and its spatial context. The expected output is a fault label (or “normal”) for every data segment.

We achieve this with HiFiNet, a two-stage architecture that:
\begin{itemize}
    \item Learns per-node representations: train an LSTM autoencoder that compresses each signal and provides an initial fault prediction at the edge.
    \item Refines with network context: periodically aggregate embeddings at the server where an Iterative Graph Network propagates attention across neighbors to produce the final fault labels.
\end{itemize}

This overview reminds readers of the task, clarifies the desired output, and maps the key steps that follow.

\subsection{Hierarchical Fault Identification Network (HiFiNet)}
\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{images/HiFiNet.png}
  \caption{Proposed HiFiNet inference pipeline, illustrating the Edge Classifier processing a target node's temperature sequence and the Network Classifier integrating edge outputs with contextual network data.}
  \label{fig:hifinet}
\end{figure}
We propose HiFiNet, a Hierarchical Fault Identification Network, for the classification of sensor faults in WSNs. HiFiNet jointly leverages per-node temporal features and network‐level interactions to outperform single-level baselines. Moving beyond conventional approaches, our key innovations lie in the novel integration of a confidence-guided Graph Attention Network. This architecture establishes a new paradigm for fault diagnosis by dynamically learning to trust and weigh information from individual sensors and the broader network context. We will demonstrate how this sophisticated approach leads to a more robust and accurate fault identification system. The overall inference pipeline is depicted in Figure~\ref{fig:hifinet}.

\subsubsection{Edge Classifier}
The first stage of HiFiNet is the Edge Classifier. This component's primary role is to analyze the time-series data from an individual target sensor node. Through this analysis, it extracts discriminative features and performs an initial fault classification. The design of the Edge Classifier involves a two-phase training process. It begins with the unsupervised pre-training of a feature extractor, which is based on a Long Short-Term Memory Stacked Autoencoder (LSTM-SAE) architecture. This is followed by a supervised fine-tuning phase for the fault classification task. The specifics of this training methodology, including the mathematical formulations for the pre-training and fine-tuning stages, are detailed in Appendix~\ref{app:edge_training}. The edge logits and the learned embedding are then forwarded to the network layer to serve as the initial node states, denoted as \(\mathcal{H}^{(0)}\).

\subsubsection{Iterative Graph Network}
The second stage of HiFiNet is the IGN, which refines the initial fault assessments or embeddings from the Edge Classifier by incorporating network-wide spatial context and inter-node dependencies. The core of the Network Classifier is an IGN. Figure~\ref{fig:ign} depicts the information flow in an IGN.

\begin{figure}
  \centering
  \includegraphics[width=0.8\linewidth]{images/IGN.png}
  \caption{Illustration of the Iterative Graph Network architecture, showing the iterative process of feature modulation, graph attention convolution, and confidence update.}
  \label{fig:ign}
\end{figure}

The IGN is designed to dynamically refine node representations by iteratively propagating and aggregating information across the graph, modulated by an evolving confidence measure. Let \(\mathcal{H}^{(0)} \in \mathbb{R}^{N \times d_0}\) be the vector representation of a node with \(N\) is the number of nodes and \(d_0\) is the dimension of the edge output. The IGN performs \(K\) iterations.

In each iteration \(k = 0, \dots, K\):
\begin{itemize}
  \item Feature Modulation (for \(k > 0\)):
    The initial node embeddings \(\mathbf{H}^{(0)}\) are modulated based on a confidence score vector \(\mathbb{c}^{(k-1)} \in \mathbb{R}^{N \times 1}\), derived from the \((k-1)\)-th iteration's GAT output. A Confidence Modulator function, \(\mathcal{M}_{\text{c}}\), is employed using Feature-wise Linear Modulation (FiLM):
    \begin{equation}
      \mathcal{H}'^{(k)} = \mathcal{M}_{\text{c}}(\mathcal{H}^{(0)}, \mathbb{c}^{(k-1)}) = g_{\gamma}(c^{(k-1)}) \odot \mathcal{H}^{(0)} + g_{\beta}(c^{(k-1)})
    \end{equation}
    where \(g_{\gamma}(\cdot)\) and \(g_{\beta}(\cdot)\) are learnable functions that generate scaling and shifting parameters from the confidence scores, and \(\odot\) denotes element-wise multiplication. For the first iteration \(k=0\), no modulation is applied, so \(\mathcal{H}'^{(0)} = \mathcal{H}^{(0)}\). The Confidence Modulator allows the network to adaptively emphasize or de-emphasize features based on the current confidence in their classification.

  \item Graph Attention Convolution (GAT) Block:
    The modulated features \(\mathbb{H}'^{(k)}\) are then processed by a GAT block, denoted as \(\mathcal{G}\). This block typically consists of one or more Graph Attention Convolution layers, which enable nodes to selectively attend to their neighbors' features when updating their own representations:
    \begin{equation}
      \mathcal{H}_{\text{G}}^{(k)} = \mathcal{G}(\mathcal{H}'^{(k)}, A)
    \end{equation}
    where \(A\) is the adjacency matrix representing the graph structure. Each GAT layer computes attention coefficients for neighboring nodes, aggregates their features weighted by these coefficients, and applies a linear transformation. Layer normalization and activation functions  are often applied between GAT layers. Dropout are also used for regularization. The output \(\mathcal{H}_{\text{G}}^{(k)} \in \mathbb{R}^{N \times d_{\text{G}}}\) contains node representations that have incorporated information from their local graph neighborhood.

  \item Temporary Classification and Confidence Update (if \(k < K-1)\):
    If it is not the final iteration, the output \(\mathcal{H}_{\text{G}}^{(k)}\) is passed through a temporary classifier, \(f_{\text{temp}}\), to obtain intermediate class logits \(z^{(k)}\) for each node:
    \begin{equation}
      z^{(k)} = f_{\text{temp}}(\mathcal{H}_{\text{G}}^{(k)})
    \end{equation}
    These logits are converted to probabilities \(P^{(k)}\) using the softmax function:
    \begin{equation}
      P^{(k)} = \text{softmax}(\mathbf{z}^{(k)})
    \end{equation}
    The confidence score \(c_i^{(k)}\) for each node \(i\) is then derived from these probabilities, typically as the maximum probability value:
    \begin{equation}
      c_i^{(k)} = \max(P_i^{(k)})
    \end{equation}
    This results in a confidence vector \(\mathbf{c}^{(k)}\), which is then used in the Feature Modulation step of the next iteration (\(k+1\)). This iterative process allows the model to progressively refine its understanding by focusing subsequent GAT operations based on the certainty of intermediate predictions.
\end{itemize}

After \(K\) iterations, the final GAT output \(\mathcal{H}_{\text{G}}^{(K_{iter}-1)}\) represents the spatially-refined node embeddings. This and the original representation are then passed to a classification head to output the network classification.
